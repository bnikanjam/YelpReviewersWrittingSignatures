{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Reviews: Authorship Attribution with Python and scikit-learn\n",
    "\n",
    "We humans almost always leave clues behind our path and writting simple texts seems to be no exception. Here we look at yelp reviews data set and and explore if we can find the author of a yelp review based on past reviews of an author. How style of a writting i.e. review can reveal its author identity?\n",
    "\n",
    "Often, it’s possible to identify someone using only their unique style of writing. We’ll see how easy it is to identify people using their writing style through machine learning. Specifically, we’ll look at reviewers who have left multiple reviews on Yelp. \n",
    "\n",
    "We’ll teach a machine learning system to differentiate between different writing styles, and then see how well it can predict the correct author of a review, looking only at the review text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVERVIEW\n",
    "\n",
    "we will:\n",
    "\n",
    "Load about 6 million reviews from the 2018 Yelp Dataset Challenge\n",
    "\n",
    "Find users who have left at least 500 reviews\n",
    "\n",
    "Train a support vector machine classifier to identify the writing style of each author\n",
    "\n",
    "See how well the classifier can identify reviews it hasn’t seen during training\n",
    "\n",
    "Relying on scikit-learn for the machine learning components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Dataset JSON Download and Documentation\n",
    "\n",
    "https://www.yelp.com/dataset/download\n",
    "\n",
    "https://www.yelp.com/dataset/documentation/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### review.json\n",
    "Contains full review text data including the user_id that wrote the review and the business_id the review is written for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.2 s, sys: 8.28 s, total: 1min 3s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "json_review_dataset = \"yelp_dataset/yelp_academic_dataset_review.json\"\n",
    "reviews = [json.loads(review) for review in open(json_review_dataset)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first review to see how the data is structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 5996996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'review_id': 'x7mDIiDB3jEiPGPHOmDzyw',\n",
       " 'user_id': 'msQe1u7Z_XuqjGoqhB0J5g',\n",
       " 'business_id': 'iCQpiavjjPzJ5_3gPD5Ebg',\n",
       " 'stars': 2,\n",
       " 'date': '2011-02-25',\n",
       " 'text': \"The pizza was okay. Not the best I've had. I prefer Biaggio's on Flamingo / Fort Apache. The chef there can make a MUCH better NY style pizza. The pizzeria @ Cosmo was over priced for the quality and lack of personality in the food. Biaggio's is a much better pick if youre going for italian - family owned, home made recipes, people that actually CARE if you like their food. You dont get that at a pizzeria in a casino. I dont care what you say...\",\n",
       " 'useful': 0,\n",
       " 'funny': 0,\n",
       " 'cool': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_cnt = len(reviews)\n",
    "print(f\"Number of reviews: {reviews_cnt}\")\n",
    "reviews[0]  # looking at 1st review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick look at star rating percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\t 2641880 \t 44%\n",
      "****\t 1335957 \t 22%\n",
      "***\t 673206 \t 11%\n",
      "**\t 487813 \t 8%\n",
      "*\t 858139 \t 14%\n",
      "CPU times: user 1.1 s, sys: 47 ms, total: 1.15 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "star_ratings_cnt = Counter([review['stars'] for review in reviews])\n",
    "for star in range(5, 0, -1):\n",
    "    star_cnt = star_ratings_cnt[star]\n",
    "    print(f\"{'*' * star}\\t {star_cnt} \\t {round(star_cnt / reviews_cnt * 100)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Most Active Reviewers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.52 s, sys: 4.89 s, total: 8.41 s\n",
      "Wall time: 8.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_prolific_reviewers = 100  # top reviewers by number of reviews to be included\n",
    "prolific_reviewers = Counter([review['user_id'] for review in reviews]).most_common(num_prolific_reviewers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of (Reviewer ID, Number of Reviews)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('CxDOIDnH8gp9KXzpBHJYXw', 3739),\n",
       " ('bLbSNkLggFnqwNNzzq-Ijw', 2229),\n",
       " ('PKEzKWv_FktMm2mGPjwd0Q', 1674),\n",
       " ('DK57YibC5ShBmqQl97CKog', 1574),\n",
       " ('QJI9OSEn6ujRCtrX06vs1w', 1324),\n",
       " ('d_TBs6J3twMy9GChqUEXkg', 1245),\n",
       " ('hWDybu_KvYLSdEFzGrniTw', 1220),\n",
       " ('ELcQDlf69kb-ihJfxZyL0A', 1204),\n",
       " ('cMEtAiW60I5wE_vLfTxoJQ', 1201),\n",
       " ('YRcaNlwQ6XXPFDXWtuMGdA', 1195),\n",
       " ('U4INQZOPSUaj8hMjLlZ3KA', 1182),\n",
       " ('62GNFh5FySkA3MbrQmnqvg', 1126),\n",
       " ('UYcmGbelzRa0Q6JqzLoguw', 1104),\n",
       " ('dIIKEfOgo0KqUfGQvGikPg', 1049),\n",
       " ('iDlkZO2iILS8Jwfdy7DP9A', 990),\n",
       " ('n86B7IkbU20AkxlFX_5aew', 956),\n",
       " ('Ry1O_KXZHGRI8g5zBR3IcQ', 935),\n",
       " ('N3oNEwh0qgPqPP3Em6wJXw', 927),\n",
       " ('rCWrxuRC8_pfagpchtHp6A', 925),\n",
       " ('WeVkkF5L39888IPPlRhNpg', 886),\n",
       " ('0BBUmH7Krcax1RZgbH4fSA', 884),\n",
       " ('U5YQX_vMl_xQy8EQDqlNQQ', 883),\n",
       " ('fiGqQ7pIGKyZ9G0RqWLMpg', 861),\n",
       " ('pMefTWo6gMdx8WhYSA2u3w', 858),\n",
       " ('Q9mA60HnY87C1TW5kjAZ6Q', 851),\n",
       " ('3nDUQBjKyVor5wV0reJChg', 843),\n",
       " ('YMgZqBUAddmFErxLtCfK_w', 822),\n",
       " ('ic-tyi1jElL_umxZVh8KNA', 819),\n",
       " ('Wc5L6iuvSNF5WGBlqIO8nw', 818),\n",
       " ('dt9IHwfuZs9D9LOH7gjNew', 802),\n",
       " ('Xxvz5g67eaCr3emnkY5M6w', 792),\n",
       " ('SlgpAnj2gQd44EM_Uq6DkQ', 779),\n",
       " ('HJj82f-csBI7jjgenwqhvw', 776),\n",
       " ('gwIqbXEXijQNgdESVc07hg', 774),\n",
       " ('PeLGa5vUR8_mcsn-fn42Jg', 774),\n",
       " ('Wu0yySWcHQ5tZ_59HNiamg', 764),\n",
       " ('RBZ_kMjowV0t6_nv2UKaDQ', 762),\n",
       " ('Wx7cbLDqYEL3_aVZwh82Ww', 758),\n",
       " ('MMf0LhEk5tGa1LvN7zcDnA', 756),\n",
       " ('4wp4XI9AxKNqJima-xahlg', 751),\n",
       " ('C2C0GPKvzWWnP57Os9eQ0w', 740),\n",
       " ('ffPY_bHX8vLebHu8LBEqfg', 734),\n",
       " ('y3FcL4bLy0eLlkb0SDPnBQ', 729),\n",
       " ('qewG3X2O4X6JKskxyyqFwQ', 729),\n",
       " ('j6wLUT0ZXi-x0otelYIFpA', 725),\n",
       " ('EiP1OFgs-XGcKZux0OKWIA', 723),\n",
       " ('6Ki3bAL0wx9ymbdJqbSWMA', 718),\n",
       " ('0tvCcnfJnSs55iB6mqPk3w', 714),\n",
       " ('M9rRM6Eo5YbKLKMG5QiIPA', 713),\n",
       " ('3nIuSCZk5f_2WWYMLN7h3w', 696),\n",
       " ('tH0uKD-vNwMoEc3Xk3Cbdg', 686),\n",
       " ('yT_QCcnq-QGipWWuzIpvtw', 669),\n",
       " ('TbhyP24zYZqZ2VJZgu1wrg', 668),\n",
       " ('bhJ6ivAuSpgXP4JiKWjSZQ', 666),\n",
       " ('A0j21z2Q1HGic7jW6e9h7A', 663),\n",
       " ('Q4Qfu-3vYtL1LRm2X1b0Gg', 660),\n",
       " ('2e5V6M4GNufEnbGJpVdCjw', 658),\n",
       " ('V-BbqKqO8anwplGRx9Q5aQ', 653),\n",
       " ('PcvbBOCOcs6_suRDH7TSTg', 651),\n",
       " ('Kj9cFO70zZOQorN0mgeLWA', 649),\n",
       " ('JLv2Dmfj73-I0d9N41tz1A', 649),\n",
       " ('S9Jw00eZHVj5_0sOM_C5Rg', 637),\n",
       " ('O3pSxv1SyHpY4qi4Q16KzA', 633),\n",
       " ('8DEyKVyplnOcSKx39vatbg', 632),\n",
       " ('Reuq65EOFI938Yg8xgff9g', 627),\n",
       " ('Lfv4hefW1VbvaC2gatTFWA', 627),\n",
       " ('ACwBMSJzgW6vOvV7vOrk8Q', 626),\n",
       " ('PGeiszoVusiv0wTHVdWklA', 624),\n",
       " ('1kNsEAhGU8d8xugMuXJGFA', 622),\n",
       " ('qlC607Cyp0Mj91vDFvkp3Q', 614),\n",
       " ('oIxsWOWytMmV4bf_ffo01w', 613),\n",
       " ('xDl9ZF3SckkZde_48W6WeA', 607),\n",
       " ('d0D7L-vfQDIADolnPAcb9A', 605),\n",
       " ('xhhE0txKwQtRzgQVVdKkvg', 604),\n",
       " ('1O638BDK_fWuxgTVJwff-A', 601),\n",
       " ('qPVtjjp8sNQ32p9860SR9Q', 600),\n",
       " ('hZfzVrhsCQ9JDAb2jYoJNQ', 599),\n",
       " ('aDYNz8cujkDdmbiOh95ANA', 596),\n",
       " ('ciXjBfJrAEteIKpzZg4I9g', 592),\n",
       " ('Z128ihQea7BLPh2T9q9sKA', 589),\n",
       " ('Tsm8VraTp5OGyVALtUiCeQ', 588),\n",
       " ('Fv0e9RIV9jw5TX3ctA1WbA', 587),\n",
       " ('V4TPbscN8JsFbEFiwOVBKw', 586),\n",
       " ('I-4KVZ9lqHhk8469X9FvhA', 586),\n",
       " ('DoRCeCcJbrsM2BiAKj3trA', 586),\n",
       " ('FREeRQtjdJU83AFtdETBBw', 583),\n",
       " ('NfU0zDaTMEQ4-X9dbQWd9A', 583),\n",
       " ('FIk4lQQu1eTe2EpzQ4xhBA', 583),\n",
       " ('7Oe6ikklTjVBbEFw9emLcA', 583),\n",
       " ('24AzZDQKHySwMQR7VQVCAg', 582),\n",
       " ('bpgbVPJcG-wQ4cNBcPPIxw', 582),\n",
       " ('kjeX2RXvW7RhBbD2QLd5jA', 580),\n",
       " ('zFYs8gSUYDvXkb6O7YkRkw', 579),\n",
       " ('1fNQRju9gmoCEvbPQBSo7w', 575),\n",
       " ('sYQyXDjGaJj7wfaqz5u8KQ', 575),\n",
       " ('oUK6Xs5dPPnP4whFeZExGg', 571),\n",
       " ('rKkpHJkJ27xOwnhy_bMkvA', 569),\n",
       " ('J3ucveGKKJDvtuCNnb_x0g', 566),\n",
       " ('qibGLHABNReGeJr2w4_8yQ', 566),\n",
       " ('5CgjjDAic2-FAvCtiHpytA', 565)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"List of (Reviewer ID, Number of Reviews)\")\n",
    "prolific_reviewers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create a balanced dataset i.e. we want the same number of reviews of each reviewer. We’ll go through all our reviews again and keep only those reviews written by the ??? authors we identified above, and only ??? reviews from each author. Below, keep_ids is a dictionary which we’ll use to keep count of how many reviews we have from each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.24 s, sys: 2.94 ms, total: 1.24 s\n",
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_reviews = 500  # max number of reviews kept for each reviewer\n",
    "keep_ids = {pr[0] : 0 for pr in prolific_reviewers}\n",
    "\n",
    "keep_reviews = []\n",
    "for review in reviews:\n",
    "    uid = review['user_id']\n",
    "    if uid in keep_ids and keep_ids[uid] < num_reviews:\n",
    "        keep_reviews.append(review)\n",
    "        keep_ids[uid] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keep_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll split the reviews we kept into two lists: \n",
    "\n",
    "    one for the texts of the reviews, \n",
    "    another for the author ids. \n",
    "    \n",
    "    \n",
    "\n",
    "The two lists are implicitly associated by index i.e. the first text in our texts array was written by the first author in our authors array). \n",
    "\n",
    "In machine learning, we refer to these as “instances” and “labels”.\n",
    "\n",
    "    Instances are the things we use to learn from, and the \n",
    "    Labels are the things we are trying to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 ms, sys: 342 ms, total: 389 ms\n",
      "Wall time: 659 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = [review['text'] for review in keep_reviews]\n",
    "authors = [review['user_id'] for review in keep_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to import some things from the scikit-learn library. Specifically, we need a vectorizer (something that transforms our texts into a numerical representation that’s easier to work with) and a classifier (the thing that learns how to discriminate based on labeled examples). \n",
    "\n",
    "We’ll be using TfidfVectorizer, which transforms our text into vectors with tf-idf weighting and a LinearSVM, which is a Support Vector Machine with a linear kernel — a kernel that is often used for text classification tasks. \n",
    "\n",
    "We’ll also import a helper function called train_test_split. We’ll use this to split our data into a \n",
    "\n",
    "    training set and a \n",
    "    test set. \n",
    "    \n",
    "The classifier will learn patterns from the training set, and then we’ll make sure that it actually works by seeing if it can correctly predict the authors in the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 µs, sys: 0 ns, total: 16 µs\n",
      "Wall time: 20 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform  the reviews / texts into vectors by setting up a vectorizer and giving it the list of our texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.66 s, sys: 113 ms, total: 6.78 s\n",
      "Wall time: 6.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vectors variable contains a sparse matrix with shape (50000, 69810) which is 100 by 500\n",
      "Number of Features: 69810\n"
     ]
    }
   ],
   "source": [
    "print( f\"Our vectors variable contains a sparse matrix with shape {vectors.shape} which is {num_prolific_reviewers} by {num_reviews}\")\n",
    "print(f\"Number of Features: {vectors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vectors variable contains a sparse matrix with shape 50000 (which is 500 texts times by 100 authors — the number of texts we kept) by 68619 which is the number of features. \n",
    "\n",
    "Our features are single words (unigrams) and each text is represented by an indication of how often that word appears in that text (which is nearly always 0 as we have 68619 unique words in the dataset).\n",
    "\n",
    "Often in machine learning, you’ll see the instances (texts in our case) referred to as Xs and the labels as ys. You can think about machine learning tasks as a function y = f(x). We have x (the review text) and we want to know y (the author’s ID). The SVM attempts to learn a function f that can map the texts to the labels. We’ll follow this convention, and break our texts into X_train (the texts we’ll show the SVM as learning examples) and X_test (the texts we won’t show to the SVM so we can see if it’s able to predict the correct authors for these texts based on the patterns it learned from the texts in X_train). We’ll similarly break our labels (the author ids) into two arrays as well: y_train and y_test. We can use the function provided by scikit-learn to handle taking a random sample of our texts and labels, while making sure that the indices still correspond, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a fixed random_state to ensure that the same random sampling is used every time the code is run.\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, authors, test_size=0.2, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 40000 texts (80% of our data) to train on and 10000 (20%) for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 69810), (10000, 69810))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING AND TESTING A CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to call fit on our classifier and pass in the learning texts and labels. Then we’ll call predict to get predictions on the test data, and look at some metrics to see how well it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 s, sys: 62.1 ms, total: 14.4 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make predictions on the test set (note that the SVM has never seen the labels from the test set that are stored in y_test). The SVM will output whichever user_id it thinks is most likely to be the author of that review, for each review we pass in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = 0\n",
    "for test, prediction in zip(predictions, y_test):\n",
    "    if prediction == test:\n",
    "        correct_prediction += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the classifier did a good job, the predictions should be similar to the test labels (y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8554"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_prediction/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8554"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPROVING OUR SYSTEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see if we can tune some parameters and do even better.\n",
    "\n",
    "The following vectorizer looks at words individually (unigrams) but also looks at pairs of words (bigrams). This makes our feature space much larger, so we’ll need a bit more processing time, both to create the vectors, and to train the SVM using the vectors. To alleviate this issue, we’ll tell the vectorizer to ignore all words and word pairs that don’t appear in at least five different reviews — there are a lot of very rarely used words, and we can’t learn anything from these anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 s, sys: 683 ms, total: 24.5 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=5)\n",
    "vectors = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we have 202611 features, about.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Now we have {vectors.shape[1]} features, about.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vectors, authors, test_size=0.2, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.1 s, sys: 186 ms, total: 45.3 s\n",
      "Wall time: 45.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)\n",
    "predictions = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can now identify the correct author correctly 89.75% of the time.\n",
      "Considering that some of the reviews are only a few sentences long, it is perhaps surprising that the writing styles are inactive enough.\n"
     ]
    }
   ],
   "source": [
    "print(f\"We can now identify the correct author correctly {accuracy_score(y_test, predictions)*100}% of the time.\\nConsidering that some of the reviews are only a few sentences long, it is perhaps surprising that the writing styles are inactive enough.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This could help find people who have more than one Yelp account for the purposes of promoting their own establishment or leaving bad reviews for competitors. It is also useful in forensic linguistics when the true authorship of someone’s will or suicide note is often questioned, and it can be used to prove the authorship of disputed literary works, such as Shakespeare’s plays or books written under pseudonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
